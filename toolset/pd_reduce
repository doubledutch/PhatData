#!/usr/bin/python

import urllib2
import json
import argparse
import sys

parser = argparse.ArgumentParser(description='Read json from stdin map it and reduce it to stdout')
parser.add_argument('--snapshot', dest='snapshot', help='Number of inputs between output of snapshots',default=1000)
parser.add_argument('--no-snapshot', dest='nosnapshot', action='store_true')
parser.add_argument('script', help='Script containing reduce function')
parser.set_defaults(nosnapshot=False)

args = parser.parse_args()

execfile(args.script)

def printAcumulator():
	obj={
		'index':count-1,
		'acumulator':acumulator
	}
	print json.JSONEncoder().encode(obj)

#TODO: we should read and write in batches ... maybe collect the input in a list?

acumulator={}
count=0
snapshotcount=0

for line in sys.stdin:
	data=json.loads(line)
	acumulator=pdreduce(data,acumulator)
	if not args.nosnapshot:
		snapshotcount=snapshotcount+1
		if snapshotcount==args.snapshot:
			printAcumulator()
			snapshotcount=0
	count=count+1

printAcumulator()